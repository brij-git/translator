{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump.\tउछलो.\n",
      "['Help!', 'बचाओ!']\n"
     ]
    }
   ],
   "source": [
    "# Assign the data path.\n",
    "data_path = \"hin.txt\"\n",
    "\n",
    "# Read in the data.\n",
    "lines = io.open(data_path, encoding = \"utf-8\").read().split(\"\\n\")\n",
    "lines  = lines[:-1]\n",
    "print(lines[1])\n",
    "# Split the data into input and target sequences.\n",
    "lines = [line.split(\"\\t\") for line in lines]\n",
    "print(lines[0])\n",
    "# We define the starting signal to be \"\\t\" and the\n",
    "# ending signal to be \"\\n\". These signals tell the\n",
    "# model that when it sees \"\\t\" it should start\n",
    "# producing its translation and produce \"\\n\" when\n",
    "# it wants to end its translation. Let us add\n",
    "# \"\\t\" to the start and \"\\n\" to the end \n",
    "# of all input and output sentences.\n",
    "lines = [(\"\\t\" + line[0] + \"\\n\", \"\\t\" + line[1] + \"\\n\") for\n",
    "            line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tबचाओ!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (lines[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out the Best Lengths of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the input and output lengths.\n",
    "input_lengths = np.array([len(line[0]) for line in lines])\n",
    "output_lengths = np.array([len(line[1]) for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869\n"
     ]
    }
   ],
   "source": [
    "print(len(input_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75.0, 80.0, 0.0, 120.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(input_lengths)\n",
    "plt.axis([75,80, 0 , 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85.0, 89.0, 0.0, 20.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(output_lengths)\n",
    "plt.axis([85,89,0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = 78\n",
    "hindi = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = []\n",
    "for i in range(len(input_lengths)):\n",
    "    if(input_lengths[i]<75 and output_lengths[i]<85):\n",
    "        line1 = line1 + [lines[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2856\n"
     ]
    }
   ],
   "source": [
    "print(len(line1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the histogram of the length of the input sentences and choose the length that makes the most sense. \n",
    "\n",
    "The reason we don't want sentences that are too long is because the computation becomes trickier for longer sentences and the performance also degrades. However we also want as many sentences in our dataset as possible.\n",
    "\n",
    "Thus it is important to choose the right length and discard sentences longer than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same for the lengths of the output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 2869  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [(line[0]) for line in line1]\n",
    "target_texts = [(line[1]) for line in line1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = set()\n",
    "target_characters = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in input_texts:\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in target_texts:\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print( len(input_characters))\n",
    "print( len(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2856\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 74\n",
      "Max sequence length for outputs: 82\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            \n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 [==============================] - 49s 1s/step - loss: 1.2814 - val_loss: 2.0101\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 36s 999ms/step - loss: 1.2184 - val_loss: 2.0068\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 1.2129 - val_loss: 1.9766\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 37s 1s/step - loss: 1.2029 - val_loss: 1.9887\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 36s 999ms/step - loss: 1.1952 - val_loss: 1.9671\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 1.1920 - val_loss: 2.0451\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 1.2503 - val_loss: 1.9707\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 36s 999ms/step - loss: 1.1775 - val_loss: 1.9477\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 1.1841 - val_loss: 1.9748\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 1.1588 - val_loss: 1.9080\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 36s 989ms/step - loss: 1.1538 - val_loss: 1.9311\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 29s 804ms/step - loss: 1.1556 - val_loss: 1.9317\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 22s 628ms/step - loss: 1.1376 - val_loss: 1.9015\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 19s 532ms/step - loss: 1.1238 - val_loss: 1.8768\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 20s 555ms/step - loss: 1.1173 - val_loss: 1.9065\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 21s 595ms/step - loss: 1.1116 - val_loss: 1.8694\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 21s 577ms/step - loss: 1.0970 - val_loss: 1.8571\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 21s 596ms/step - loss: 1.1081 - val_loss: 1.8727\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 22s 629ms/step - loss: 1.0911 - val_loss: 1.9247\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 25s 693ms/step - loss: 1.0925 - val_loss: 1.8887\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 23s 653ms/step - loss: 1.0835 - val_loss: 1.8247\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 23s 651ms/step - loss: 1.0624 - val_loss: 1.8395\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 1.0550 - val_loss: 1.8146\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 22s 599ms/step - loss: 1.0831 - val_loss: 1.8386\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 22s 607ms/step - loss: 1.0577 - val_loss: 1.8275\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 22s 600ms/step - loss: 1.0329 - val_loss: 1.7916\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 20s 563ms/step - loss: 1.0281 - val_loss: 1.7575\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 21s 584ms/step - loss: 1.0148 - val_loss: 1.7553\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 21s 574ms/step - loss: 0.9955 - val_loss: 1.7522\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 23s 652ms/step - loss: 0.9943 - val_loss: 1.7500\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 21s 596ms/step - loss: 0.9930 - val_loss: 1.5898\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 19s 537ms/step - loss: 0.9727 - val_loss: 1.6310\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 20s 554ms/step - loss: 0.9656 - val_loss: 1.7869\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 20s 551ms/step - loss: 0.9579 - val_loss: 1.7701\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 20s 555ms/step - loss: 0.9461 - val_loss: 1.7370\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 20s 555ms/step - loss: 0.9313 - val_loss: 1.6417\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 25s 705ms/step - loss: 0.9242 - val_loss: 1.6716\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.9322 - val_loss: 1.5861\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.9112 - val_loss: 1.6347\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.9075 - val_loss: 1.6314\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 39s 1s/step - loss: 0.8959 - val_loss: 1.6308\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.9124 - val_loss: 1.6498\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.8877 - val_loss: 1.5096\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.8808 - val_loss: 1.4429\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.8728 - val_loss: 1.5737\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 38s 1s/step - loss: 0.8678 - val_loss: 1.6009\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 36s 991ms/step - loss: 0.8692 - val_loss: 1.5598\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 37s 1s/step - loss: 0.8681 - val_loss: 1.6365\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 25s 684ms/step - loss: 0.8559 - val_loss: 1.4286\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 16s 458ms/step - loss: 0.8526 - val_loss: 1.6046\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 20s 554ms/step - loss: 0.8502 - val_loss: 1.4369\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 21s 591ms/step - loss: 0.8462 - val_loss: 1.5174\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 21s 575ms/step - loss: 0.8390 - val_loss: 1.6070\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 21s 578ms/step - loss: 0.8352 - val_loss: 1.5265\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 19s 518ms/step - loss: 0.8314 - val_loss: 1.4232\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 18s 516ms/step - loss: 0.8223 - val_loss: 1.4872\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 19s 520ms/step - loss: 0.8181 - val_loss: 1.5133\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 19s 523ms/step - loss: 0.8139 - val_loss: 1.4179\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 19s 529ms/step - loss: 0.8119 - val_loss: 1.4367\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 18s 511ms/step - loss: 0.8143 - val_loss: 1.4849\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 19s 519ms/step - loss: 0.8058 - val_loss: 1.4464\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 18s 489ms/step - loss: 0.8033 - val_loss: 1.4711\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 18s 504ms/step - loss: 0.7952 - val_loss: 1.5004\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 18s 512ms/step - loss: 0.7959 - val_loss: 1.5055\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 18s 514ms/step - loss: 0.7914 - val_loss: 1.4756\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 19s 543ms/step - loss: 0.7886 - val_loss: 1.4119\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 19s 517ms/step - loss: 0.7862 - val_loss: 1.4963\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 18s 508ms/step - loss: 0.7823 - val_loss: 1.4404\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 18s 508ms/step - loss: 0.7748 - val_loss: 1.4039\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 18s 500ms/step - loss: 0.7747 - val_loss: 1.4052\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 20s 545ms/step - loss: 0.7705 - val_loss: 1.4608\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 18s 511ms/step - loss: 0.8011 - val_loss: 1.5895\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 19s 539ms/step - loss: 0.7989 - val_loss: 1.4432\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 26s 734ms/step - loss: 0.7710 - val_loss: 1.3845\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 36s 989ms/step - loss: 0.7700 - val_loss: 1.3407\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 760s 22s/step - loss: 0.7577 - val_loss: 1.4225\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 18s 508ms/step - loss: 0.7557 - val_loss: 1.5560\n",
      "Epoch 78/100\n",
      "36/36 [==============================] - 17s 478ms/step - loss: 0.7502 - val_loss: 1.4374\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 16s 455ms/step - loss: 0.7476 - val_loss: 1.3649\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 16s 458ms/step - loss: 0.7419 - val_loss: 1.4196\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 21s 591ms/step - loss: 0.7420 - val_loss: 1.3629\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - 35s 986ms/step - loss: 0.7424 - val_loss: 1.3866\n",
      "Epoch 83/100\n",
      "36/36 [==============================] - 36s 992ms/step - loss: 0.7325 - val_loss: 1.3590\n",
      "Epoch 84/100\n",
      "36/36 [==============================] - 36s 994ms/step - loss: 0.7307 - val_loss: 1.3952\n",
      "Epoch 85/100\n",
      "36/36 [==============================] - 35s 985ms/step - loss: 0.7289 - val_loss: 1.3476\n",
      "Epoch 86/100\n",
      "36/36 [==============================] - 36s 998ms/step - loss: 0.7267 - val_loss: 1.3668\n",
      "Epoch 87/100\n",
      "36/36 [==============================] - 35s 981ms/step - loss: 0.7289 - val_loss: 1.3535\n",
      "Epoch 88/100\n",
      "36/36 [==============================] - 36s 998ms/step - loss: 0.7189 - val_loss: 1.3398\n",
      "Epoch 89/100\n",
      "36/36 [==============================] - 36s 996ms/step - loss: 0.7244 - val_loss: 1.3756\n",
      "Epoch 90/100\n",
      "36/36 [==============================] - 36s 998ms/step - loss: 0.7147 - val_loss: 1.3257\n",
      "Epoch 91/100\n",
      "36/36 [==============================] - 35s 967ms/step - loss: 0.7110 - val_loss: 1.3563\n",
      "Epoch 92/100\n",
      "36/36 [==============================] - 29s 818ms/step - loss: 0.7075 - val_loss: 1.3631\n",
      "Epoch 93/100\n",
      "36/36 [==============================] - 28s 776ms/step - loss: 0.7071 - val_loss: 1.3744\n",
      "Epoch 94/100\n",
      "36/36 [==============================] - 32s 896ms/step - loss: 0.7030 - val_loss: 1.3008\n",
      "Epoch 95/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 0.7010 - val_loss: 1.3696\n",
      "Epoch 96/100\n",
      "36/36 [==============================] - 36s 996ms/step - loss: 0.6998 - val_loss: 1.3217\n",
      "Epoch 97/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 0.6981 - val_loss: 1.3381\n",
      "Epoch 98/100\n",
      "36/36 [==============================] - 36s 1s/step - loss: 0.6932 - val_loss: 1.3446\n",
      "Epoch 99/100\n",
      "36/36 [==============================] - 36s 999ms/step - loss: 0.6937 - val_loss: 1.3376\n",
      "Epoch 100/100\n",
      "36/36 [==============================] - 36s 999ms/step - loss: 0.6870 - val_loss: 1.4150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2238e3ea490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s2s.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "\tHelp!\n",
      "\n",
      "यह किताब है।\n",
      "\n",
      "-\n",
      "\tJump.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tJump.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tJump.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tHello!\n",
      "\n",
      "यह किताब है।\n",
      "\n",
      "-\n",
      "\tHello!\n",
      "\n",
      "यह किताब है।\n",
      "\n",
      "-\n",
      "\tCheers!\n",
      "\n",
      "उसने मुझे को को को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tCheers!\n",
      "\n",
      "उसने मुझे को को को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tGot it?\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tI'm OK.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tAwesome!\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tCome in.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tGet out!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tGo away!\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tGoodbye!\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tPerfect!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tPerfect!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tWelcome.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tWelcome.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHave fun.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tHave fun.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tHave fun.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tI forgot.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI forgot.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'll pay.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm fine.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm full.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tLet's go!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tAnswer me.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tBirds fly.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tExcuse me.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tFantastic!\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tI fear so.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI laughed.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm bored.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm broke.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm tired.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tIt's cold.\n",
      "\n",
      "यह किताब है।\n",
      "\n",
      "-\n",
      "\tWho knows?\n",
      "\n",
      "तुम कर को बहुत कर सकता है।\n",
      "\n",
      "-\n",
      "\tWho knows?\n",
      "\n",
      "तुम कर को बहुत कर सकता है।\n",
      "\n",
      "-\n",
      "\tWho knows?\n",
      "\n",
      "तुम कर को बहुत कर सकता है।\n",
      "\n",
      "-\n",
      "\tWho knows?\n",
      "\n",
      "तुम कर को बहुत कर सकता है।\n",
      "\n",
      "-\n",
      "\tWonderful!\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tBirds sing.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tCome on in.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tDefinitely!\n",
      "\n",
      "यह किताब है।\n",
      "\n",
      "-\n",
      "\tDon't move.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tFire burns.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tFollow him.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tI am tired.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI can swim.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI can swim.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI love you.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI will try.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm coming.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm hungry!\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'm hungry!\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tLet him in.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tLet him in.\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tLet me out!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tOnce again.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tPlease sit.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tWhat's new?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tWhat's new?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tWho's that?\n",
      "\n",
      "हम को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tDon't shout.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tDon't shout.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tHe stood up.\n",
      "\n",
      "उसने मुझे को को को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tHe's strong.\n",
      "\n",
      "उसने मुझे को को को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHow are you?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tI am hungry.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI like both.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI like cake.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI like dogs.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI like math.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI'll attend.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tNobody came.\n",
      "\n",
      "तुम कर को बहुत कर सकता है।\n",
      "\n",
      "-\n",
      "\tWas I wrong?\n",
      "\n",
      "हम को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tWhat's this?\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tAre you sick?\n",
      "\n",
      "तुम्हारे पास को को को से को को है।\n",
      "\n",
      "-\n",
      "\tBring him in.\n",
      "\n",
      "मुझे उसके पास कर को किया।\n",
      "\n",
      "-\n",
      "\tCome with us.\n",
      "\n",
      "हम रहा है।\n",
      "\n",
      "-\n",
      "\tHappy Easter!\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tHas Tom left?\n",
      "\n",
      "मुझे उसके पास कर को किया है।\n",
      "\n",
      "-\n",
      "\tHe is French.\n",
      "\n",
      "उसने मुझे को को को को को को को से नहीं है।\n",
      "\n",
      "-\n",
      "\tI am at home.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI can't move.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI don't know.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI don't know.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n",
      "-\n",
      "\tI have a car.\n",
      "\n",
      "मैं तुम्हें को किताब है।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    \n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
